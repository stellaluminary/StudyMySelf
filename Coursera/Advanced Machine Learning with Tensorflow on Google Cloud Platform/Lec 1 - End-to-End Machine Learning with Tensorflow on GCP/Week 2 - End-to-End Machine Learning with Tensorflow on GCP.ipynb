{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - End-to-End Machine Learning with Tensorflow on GCP\n",
    "\n",
    "URL : https://www.coursera.org/learn/end-to-end-ml-tensorflow-gcp/home/week/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating a dataset\n",
    "\n",
    "## Building an ML model involves:\n",
    "1. Create the dataset\n",
    "2. Build the model\n",
    "3. Operationalize the model\n",
    "\n",
    "## What makes a feature \"good\"?\n",
    "\n",
    "1. Be related to the objective\n",
    "2. Be known at prediction-time\n",
    "3. Be numeric with meaningful magnitude\n",
    "4. Have enough examples\n",
    "5. Bring human insight to problem\n",
    "\n",
    "## The simplest option is to sample rows randomly\n",
    "- Each data point is a birth record from the natality dataset\n",
    "- Random sampling eliminates potential biases due to order of the training examples but ...\n",
    "\n",
    "## Also ... what about triplets?\n",
    "- 3 rows with essentially the same data!\n",
    "- How can we make this data unique?\n",
    "- How can we solve this?\n",
    "\n",
    "## Solution: Split a dataset into training/validation using hashing and modulo operators\n",
    "## Developing the ML model software on the entire dataset can be expensive; you want to develop on a smaller sample\n",
    "- Develop your Tensorflow code on a small subset of data, then scale it out to the cloud\n",
    "\n",
    "## Solution: Sampling the split so that we have a small dataset to develop our code on\n",
    "- RAND() => random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "amenable 1. 말을 잘 듣는; …을 잘 받아들이는   2. (특정한 방식으로) 처리할 수 있는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Quiz\n",
    "1. True or False - In ML, you could train using all your data and decide not to hold out a test set and still get a good model\n",
    " > <font color='red'>True</font>, False\n",
    "2. What are the benefits of using the hashing and modulo operators for creating ML datasets ?\n",
    "> 1.<font color='red'>It allows you to create datasets in a repeatable manner.</font>\n",
    "> 2. It is more computationally efficient than using the rand() function.\n",
    "> 3. It provides the best performing split for training and evaluation.\n",
    "> 4. None of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hold out 1. (특히 어려운 상황에서) 지속되다   2. (어려운 상황에서) 저항하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modlue Quiz\n",
    "\n",
    "1. Numeric\n",
    "2. 3 above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hands-on Lab 2\n",
    "\n",
    "## Lab 2: Create a sample dataset\n",
    "\n",
    "### What you learn In this lab, you will learn how to:\n",
    "\n",
    "- Sample a BigQuery dataset to create datasets for ML\n",
    "- Preprocess data using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deploy 1. (군대·무기를) 배치하다   2. 효율적으로 사용하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-training-demos-ml'\n",
    "PROJECT = 'cloud-training-demos'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "\n",
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi\n",
    "\n",
    "# Create SQL query using natality data after the year 2000\n",
    "from google.cloud import bigquery\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  ABS(FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING)))) AS hashmonth\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE year > 2000\n",
    "\"\"\"\n",
    "\n",
    "df = bigquery.Client().query(\"SELECT hashmonth, COUNT(weight_pounds) AS num_babies FROM (\" + query + \") GROUP BY hashmonth\").to_dataframe()\n",
    "df.head()\n",
    "\n",
    "df.shape\n",
    "\n",
    "trainQuery = \"SELECT * FROM (\"+query+\") WHERE MOD(hashmonth, 4) < 3 AND RAND() < 0.0005\"\n",
    "evalQuery = \"SELECT * FROM (\"+query+\") WHERE MOD(hashmonth, 4) = 3 AND RAND() < 0.0005\"\n",
    "traindf = bigquery.Client().query(trainQuery).to_dataframe()\n",
    "evaldf = bigquery.Client().query(evalQuery).to_dataframe()\n",
    "print(len(traindf), len(evaldf))\n",
    "\n",
    "import pandas as pd\n",
    "def preprocess(df):\n",
    "  df = df[df.weight_pounds > 0]\n",
    "  df = df[df.mother_age > 0]\n",
    "  df = df[df.gestation_weeks > 0]\n",
    "  df = df[df.plurality > 0]\n",
    "  \n",
    "  twins_etc = dict(zip([1,2,3,4,5],['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)']))\n",
    "  df['plurality'].replace(twins_etc, inplace=True)\n",
    "  \n",
    "  nous = df.copy(deep=True)\n",
    "  nous.loc[nous['plurality'] != 'Single(1)', 'plurality'] = 'Multiple(2+)'\n",
    "  nous['is_male'] = 'Unknown'\n",
    "  \n",
    "  return pd.concat([df, nous])\n",
    "  \n",
    "traindf = preprocess(traindf)\n",
    "evaldf = preprocess(evaldf)\n",
    "traindf.head()\n",
    "\n",
    "traindf.to_csv('train.csv', index=False, header=False)\n",
    "evaldf.to_csv('eval.csv', index=False, header=False)\n",
    "\n",
    "%%bash\n",
    "wc -l *.csv\n",
    "head *.csv\n",
    "tail *.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: demo and review\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the model\n",
    "\n",
    "### Tensorflow is an open-source high-performance library for numerical computation that uses directed graph\n",
    "- Nodes represent mathematical operations\n",
    "- Edges represent arrays of data\n",
    "### A tensor is an N-dimensional array of data\n",
    "### Tensorflow toolkit hierachy\n",
    "### Working with Estimator API\n",
    "- Set up machine learning model\n",
    " 1. Regression or classification?\n",
    " 2. What is the label?\n",
    " 3. What are the features?\n",
    "- Carry out ML steps\n",
    " 1. Train the model\n",
    " 2. Evaluate the model\n",
    " 3. Predict with the model\n",
    "\n",
    "Square footage => My model => Price\n",
    "### Structure of an Estimator API ML model\n",
    "### Encoding categorical data to supply to a DNN\n",
    "- 1a. If you know the complete vocabulary beforehand:\n",
    "```python\n",
    "tf.feature_column.categorical_column_with_vocabulary_list('zipcode',vocabulary_list = ['83452','72345','87654','98723','23451'])\n",
    "```\n",
    "- 1b. If your data is already indexed; i.e., has integers in [0-N):\n",
    "\n",
    "```python\n",
    "tf.feature_column.categorical_column_with_identity('stateId',num_buckets=50)\n",
    "```\n",
    "\n",
    "- 2. To pass in a categorical column into a DNN, one option is to one-hot encode it\n",
    "\n",
    "```python\n",
    "tf.feature_column.indicator_column(my_categorical_column)\n",
    "```\n",
    "### To read CSV files, create a TextLineDataset giving it a function to decode the CSV into features, labels\n",
    "\n",
    "- dataset = tf.data.TextLineDataset(filename).map(decode_csv function)\n",
    "\n",
    "### Shuffling is important for distributed training\n",
    "```python\n",
    "dataset = dataset.shuffle(buffer_size = 10*batch_size)\n",
    "dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "dataset.make_one_shot_iterator().get_next()\n",
    "```\n",
    "\n",
    "### Estimator API comes with a method that handles distributed training and evaluation\n",
    "```python\n",
    "estimator = tf.estimator.LinearRegressor(model_dir=output_dir,\n",
    "                                         feature_columns=feature_cols)\n",
    "tf.estimator.train_and_evaluate(estimator,\n",
    "                                train_spec,\n",
    "                                eval_spec)\n",
    "```\n",
    "1. Distribute the graph\n",
    "2. Share variables\n",
    "3. Evaluate occasionally\n",
    "4. Handle machine failures\n",
    "5. Create checkpoint files\n",
    "6. Recover from failures\n",
    "7. Save summaries for TensorBoard\n",
    "\n",
    "### TrainSpec consists of the things that used to be passed into the train() method\n",
    "```python\n",
    "train_spec = tf.estimator.TrainSpec(input_fn = read_dataset('gs://.../train*',\n",
    "                                                            mode = tf.contrib.learn.ModeKeys.TRAIN),\n",
    "                                    max_steps=num_train_steps)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "```\n",
    "\n",
    "### EvalSpec controls the evaluation and the checkpointing of the model because they happen at the same time\n",
    "```python\n",
    "exporter=...\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=read_dataset('gs://.../valid*,\n",
    "                                                        mode=tf.contrib.learn.ModeKeys.EVAL),\n",
    "                                  steps=None,\n",
    "                                  start_delay_secs=60,\n",
    "                                  throttle_secs=600,\n",
    "                                  exporters=exporter)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hands-on-Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\n",
    "TRAIN_STEPS = 1000\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename_pattern, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(line_of_text):\n",
    "      # TODO #1: Use tf.decode_csv to parse the provided line\n",
    "      columns = tf.decode_csv(line_of_text, record_defaults=DEFAULTS)      \n",
    "      # TODO #2: Make a Python dict.  The keys are the column names, the values are from the parsed data\n",
    "      features = dict(zip(CSV_COLUMNS, columns))      \n",
    "      # TODO #3: Return a tuple of features, label where features is a Python dict and label a float\n",
    "      label = features.pop(LABEL_COLUMN)      \n",
    "      return features, label\n",
    "    \n",
    "    # TODO #4: Use tf.gfile.Glob to create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename_pattern)\n",
    "    print(file_list)\n",
    "    # Create dataset from file list\n",
    "    dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
    "                 .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    \n",
    "    # TODO #5: In training mode, shuffle the dataset and repeat indefinitely\n",
    "    #                (Look at the API for tf.data.dataset shuffle)\n",
    "    #          The mode input variable will be tf.estimator.ModeKeys.TRAIN if in training mode\n",
    "    #          Tell the dataset to provide data in batches of batch_size \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None\n",
    "        dataset = dataset.shuffle(buffer_size = 10*batch_size)\n",
    "    else:\n",
    "        num_epochs = 1\n",
    "    \n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    # This will now return batches of features, label\n",
    "    return dataset\n",
    "  return _input_fn\n",
    "\n",
    "# Define feature columns\n",
    "def get_categorical(name, value):\n",
    "    return tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(name, value))\n",
    "\n",
    "def get_cols():\n",
    "    return [\\\n",
    "            get_categorical('is_male', ['True','False','Unknown']),\n",
    "            tf.feature_column.numeric_column('mother_age'),\n",
    "            get_categorical('plurality',['Single(1)', 'Twins(2)', 'Triplets(3)',\n",
    "                       'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']),            \n",
    "            tf.feature_column.numeric_column('gestation_weeks')            \n",
    "           ]\n",
    "\n",
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'is_male': tf.placeholder(tf.string, [None]),\n",
    "        'mother_age': tf.placeholder(tf.float32, [None]),\n",
    "        'plurality': tf.placeholder(tf.string, [None]),\n",
    "        'gestation_weeks': tf.placeholder(tf.float32, [None])\n",
    "    }\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(output_dir):\n",
    "  EVAL_INTERVAL = 300\n",
    "  run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
    "                                      keep_checkpoint_max = 3)\n",
    "  # TODO #1: Create your estimator\n",
    "  estimator = tf.estimator.DNNRegressor(\n",
    "                       model_dir = output_dir,\n",
    "                       feature_columns = get_cols(),\n",
    "                       hidden_units = [64, 32],\n",
    "                       config = run_config)\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "                       # TODO #2: Call read_dataset passing in the training CSV file and the appropriate mode\n",
    "                       input_fn = read_dataset('train.csv', mode = tf.estimator.ModeKeys.TRAIN),\n",
    "                       max_steps = TRAIN_STEPS)\n",
    "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "                       # TODO #3: Call read_dataset passing in the evaluation CSV file and the appropriate mode\n",
    "                       input_fn = read_dataset('eval.csv', mode = tf.estimator.ModeKeys.EVAL),\n",
    "                       steps = None,\n",
    "                       start_delay_secs = 60, # start evaluating after N seconds\n",
    "                       throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
    "                       exporters = exporter)\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "# Run the model\n",
    "shutil.rmtree('babyweight_trained', ignore_errors = True) # start fresh each time\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "train_and_evaluate('babyweight_trained')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20a",
   "language": "python",
   "name": "tf20a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
