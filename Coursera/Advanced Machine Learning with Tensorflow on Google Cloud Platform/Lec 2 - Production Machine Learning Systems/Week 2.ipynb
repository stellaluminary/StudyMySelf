{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapting to Data\n",
    "\n",
    "### Agenda\n",
    "- Adapting to Data\n",
    "- Mitigating Training-Serving Skew Through Design\n",
    "- Debugging a Production Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mitigate 완화시키다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of these is least likely to change?\n",
    "\n",
    "1. An upstream model\n",
    "2. A data source maintained by another team\n",
    "3. The relationship between features and labels\n",
    "4. The distribution of inputs\n",
    "\n",
    "=> All of them can and often do change\n",
    "\n",
    "### Decoupled upstream data product\n",
    "- Input -> Model -> Output\n",
    "- Web Dev Team - Logs\n",
    " - Site Reliability(O)\n",
    " - Anti Abuse(O)\n",
    " - Data Science() - New Features\n",
    "  - Rigorously assess all features\n",
    "\n",
    "### Distributions change\n",
    "interpolation is much easier than extrapolation\n",
    "\n",
    "- Monitor descriptive statistics for your inputs and outputs\n",
    "- Monitor your residuals as a function of your inputs\n",
    "- Use custom weights in your loss function to emphasize data recency\n",
    "- Use dynamic training architecture and regularly retrain your model\n",
    "\n",
    "### Exercise : Adapting to Data\n",
    "\n",
    "- __Scenario 1: Code Sprint__\n",
    "- __Scenario 2: A Gift Horse__\n",
    " - smell\n",
    " \n",
    "### Right and Wrong Data Decision\n",
    "- patient age\n",
    "- gender\n",
    "- prior medical conditions\n",
    "- hospital name\n",
    "- vital signs\n",
    "- test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extrapolation 1. 외삽법, 보외법   2. (기지의 사실에서의) 추정 ((from))<br>\n",
    "recency : 최신, 새로움(recentness) ((of))<br>\n",
    "vigilant 바짝 경계하는, 조금도 방심하지 않는<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage\n",
    "\n",
    "### Predict political affiliation from metaphors\n",
    "\n",
    "__Solution: Cross-contamination: you have to split by author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "affiliation 1. (개인의 정치·종교적) 소속   2. (단체의) 제휴, 가맹<br>\n",
    "suspicious 1. (불법·부정행위를 한 것으로) 의혹을 갖는, 수상쩍어 하는   2. 의심스러운, 수상쩍은<br>\n",
    "contamination 1. [U] 오염(pollution), 오탁; 더러움; [C] 오탁물; [비유] 타락   2. 독가스[방사능]에 의한 오염 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Failure\n",
    "\n",
    "Rollback Initiated Version 1.0.1 Threee months old\n",
    "\n",
    "### Feedback Loops\n",
    "- Clinet - Static Model - Stale Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stale 1. 신선하지 않은, (만든 지) 오래된   2. 퀴퀴한, (좋지 못한) 냄새가 나는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Training-Serving Skew\n",
    "\n",
    "1. A discrepancy between how you handle data in the training and serving pipelines\n",
    "2. A change in the data between when you train and when you serve\n",
    "3. A feedback loop between your model and your algorithm\n",
    "\n",
    "### How Code Can Create Training/Serving Skew\n",
    "- Different library versions that are functionally equivalent but optimized differently\n",
    "- Different library versions that are not functionally equivalent\n",
    "- Re-implemented functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discrepancy (같아야 할 것들 사이의) 차이<br>\n",
    "polymorphism 1. 동질 이상(同質異像)   2. 다형(多形)(현상), 다형성; 다형 현상 ((동종 집단 가운데에서..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CSV File & Cloud Pub/Sub\n",
    " - Cloud DataFlow(Batch) & Cloud DataFlow(Streaming) \n",
    "  - Multiple CSV Files & BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging a Production Model\n",
    "\n",
    "1. Multiple Purchase orders -> Cloud Pub/Sub \n",
    "2. 1)Cloud Dataflow -> Predicted demand -> Purchasing system 2) Model\n",
    "3. 1) Google BigQuery Data Warehouse -> Cloud ML engine Model training ->(deploy)-> Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Catastrophe 1\n",
    "\n",
    "### An Actual Feedback Loop\n",
    "- Bad Data -> ML Model -> Predicts Low Demand -> Product Turnover Increases -> ML Model Loop\n",
    "\n",
    "### Business Catastrophe 2\n",
    "- Centralized Purchasing\n",
    "\n",
    "### Business Catastrophe 3\n",
    "- Solution : Stop automatic model deplyment process -> contaminated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uptick 약간의 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Keep humans in the loop\n",
    "2. Prioritize maintainability\n",
    "3. Get ready to roll back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Adaptable ML Systems\n",
    "1. Which of the following models are susceptible to a feedback loop?\n",
    "> A book-recommendation model that suggests novels its users may like based on their popularity (i.e., the number of times the books have been purchased).<br>\n",
    "A face-attributes model that detects whether a person is smiling in a photo, which is regularly trained on a database of stock photography that is automatically updated monthly.<br>\n",
    "An election-results model that forecasts the winner of a mayoral race by surveying 2% of voters after the polls have closed.<br>\n",
    "A university-ranking model that rates schools in part by their selectivity—the percentage of students who applied that were admitted.<br>\n",
    "A traffic-forecasting model that predicts congestion at highway exits near the beach, using beach crowd size as one of its feature.<br>\n",
    "A housing-value model that predicts house prices, using size (area in square meters), number of bedrooms, and geographic location as features.\n",
    ">>A book-recommendation model that suggests novels its users may like based on their popularity (i.e., the number of times the books have been purchased).<br>\n",
    "A university-ranking model that rates schools in part by their selectivity—the percentage of students who applied that were admitted.<br>\n",
    "A traffic-forecasting model that predicts congestion at highway exits near the beach, using beach crowd size as one of its feature.<br>\n",
    "2. Suppose you are building a ML-based system to predict the likelihood that a customer will leave a positive review. The user interface by which customers leave reviews changed a few months ago, but you don't know about this. Which of these is a potential consequence of mismanaging this data dependency?\n",
    "> Losses in prediction quality<br>\n",
    "Change in model serving signature<br>\n",
    "Change in ability of model to be part of a streaming ingest\n",
    ">>Losses in prediction quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn how to ...\n",
    "1. Identify performance consideration for ML models\n",
    "2. Choose appropriate ML infrastructure\n",
    "3. Select a distribution strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "- Distributed training\n",
    "- Faster input pipelines\n",
    "- Data parallelism(All Reduce)\n",
    "- Parameter Server approach\n",
    "- Inference\n",
    "\n",
    "### High performance ML\n",
    "- One key aspect is the time taken to train a model\n",
    "\n",
    "### Optimizing your Training Budget\n",
    "- time, cost, scale\n",
    "\n",
    "### Model Training can take a long time\n",
    "### Analyze Benifit of Model vs Running Cost\n",
    "- Optimize training dataset size\n",
    "- Choosing optimized infrastructure\n",
    "- Use earlier model checkpoints\n",
    "- Tuning Performance to reduce training time, reduce cost, and increase scale\n",
    "\n",
    "|Constraint|Input/Output|CPU|Memory|\n",
    "|---|---|---|---|\n",
    "|Commonly <br>Occurs|Large inputs<br> Input requires parsing <br>Small model|Expensive computations <br>UnderposeredHardware|Large number of inputs <br>Complex model|\n",
    "|Take <br>Action|Store efficiently <br>Parallelize reads <br>Consider batch size|Train on faster accel.<br> Upgrade processor <br>Run on TPUs <br>Simplify model|Add more memory <br>Use fewer layers <br>Reduce batch size|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heterogeneous : 여러 다른 종류들로 이뤄진"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing your Batch Prediction\n",
    "\n",
    "- time, cost, scale\n",
    "\n",
    "### Optimizing your Online Prediction\n",
    "\n",
    "- different : Single-Machine, Microservice, QPS,\n",
    "\n",
    "### Improving performance also adds complexity\n",
    "- Heterogenous system, Distributed systems, Model architectures\n",
    "\n",
    "### Heterogenous system require our code to work anywhere\n",
    "- CPU, GPU, TPU, Android/iOS, Edge TPU, Raspberry Pi\n",
    "\n",
    "### Deep learning works because datasets are large, but the compute required keeps increasing\n",
    "### Large models could have millions of weights\n",
    "### Training can take a long time\n",
    "### How can you make model training faster\n",
    "### Scaling with Distributed Training\n",
    "\n",
    "### Adding a single accelerator\n",
    "- Multi-core CPU -> GPU, TPU\n",
    "### Adding many machines with many possible devices\n",
    "\n",
    "### Two approaches to Data Parallelism\n",
    "1. Parameter server\n",
    "2. Sync Allreduce\n",
    "\n",
    "### Async Parameter Server\n",
    "### Sync Allreduce Architecture\n",
    "\n",
    "### Consider Async Parameter Server if...\n",
    "- many low-power or unreliable workers\n",
    "- More mature approach\n",
    "- Constrained I/O\n",
    "\n",
    "### Consider Sync Allreduce if...\n",
    "- Multiple devices on one host Fast <br> devices with strong links(e.g. TPUs)\n",
    "- Better for multiple GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3. Faster Input pipelines\n",
    "\n",
    "Training Data - Input pipeline(Bottleneck) -> Multiple GPU/TPU\n",
    "\n",
    "# Reading Data into Tensorflow\n",
    "1. Directly feed from Python\n",
    "2. Native Tensorflow Ops\n",
    "3. Read transformed tf records\n",
    "\n",
    "### Input pipeline as an ETL Process\n",
    "- Extract -> Transform -> Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-4 Data parallelism(All Reduce)\n",
    "\n",
    "### Data paralelism is a way to increase training throughput\n",
    "### Distribution API Strategy\n",
    "- Easy to use\n",
    "- Fast to train\n",
    "\n",
    "### Training with Estimator API\n",
    "### Mirrored Strategy\n",
    "- No change to the model or training loop\n",
    "- No change to input function (requires tf.data.Dataset)\n",
    "- Checkpoints and summaries are seamless\n",
    "\n",
    "# Parameter Server approach\n",
    "### Model parallelism lets you distribute a model across GPUs\n",
    "### Large embeddings need multiple machines to map sparse data\n",
    "### Estimator train_and_evaluate() handles all this\n",
    "### Estimator contains the implementation of three function - training, eval, serving\n",
    "### By encapsulating details about sessions and graphs, it also supports exporting the model for serving\n",
    "### train_and_evaluate bundles together a distributed workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seamless 1. 솔기가 없는   2. (중간에 끊어짐이 없이) 아주 매끄러운, 천의무봉의<br>\n",
    "sparse data 희소 데이터 ( 차원/전체 공간에 비해 데이터가 있는 공간이 매우 협소한 데이터를 의미합니다)<br>\n",
    "sparse (흔히 넓은 지역에 분포된 정도가) 드문, (밀도가) 희박한<br>\n",
    "configuration 1. 배열, 배치; 배열 형태   2. 환경 설정<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-5 Inference\n",
    "\n",
    "- Aspects of performance during inference - QPS, Microservice, Cost\n",
    "### Implementation Options\n",
    "- REST/HTTP API - For Streaming Pipelines\n",
    "- Cloud Machine Learning Engine - For Batch Pipelines\n",
    "- Cloud Dataflow - For Batch and Streaming Pipelines\n",
    "### Batch = Bounded Dataset\n",
    "### Performance for Batch Pipelines\n",
    "- CMLE + Microbatching : Best Option for maintainability and speed\n",
    "- SavedModel : Best option for high-speed inference below some limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enrich 1. 질을 높이다, 풍요롭게 하다; (식품에 어떤 영양소를) 강화하다   2. (더) 부유하게 만들다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing High-performance ML systems\n",
    "\n",
    "1. Machine learning training performance can be bound by:\n",
    "> Input/output speed, Read latency, Number of data points, Number of open ports, Computation speed, Memory\n",
    ">> Input/output speed,  Computation speed,  Memory\n",
    "\n",
    "2. If each of your examples is large in terms of size, requires parsing and your model is relatively simple and shallow, then your model is likely to be\n",
    ">I/O bound, so you should look for ways to store data more efficiently and ways to parallelize the reads, CPU-bound, so you should use GPUs or TPUs, Latency-bound, so you should use faster hardware\n",
    ">> I/O bound, so you should look for ways to store data more efficiently and ways to parallelize the reads\n",
    "\n",
    "3. For the fastest I/O performance in TensorFlow\n",
    ">Read TF records into your model, Read in parallel threads, Use fused operations, prefetch the data\n",
    ">> Read TF records into your model, Read in parallel threads, Use fused operations, prefetch the data\n",
    "\n",
    "4. Consider Sync All Reduce if\n",
    "> You have many distributed workers, You have a single machine that has multiple devices with fast interconnect\n",
    ">> You have a single machine that has multiple devices with fast interconnect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introduction\\\n",
    "\n",
    "## __Learn how to__\n",
    "\n",
    "- Build hybrid cloud machine learning models\n",
    "- Optimize Tensorflow graphs for mobile\n",
    "\n",
    "### Agenda\n",
    "- Kubeflow for hybrid cloud\n",
    "- Optimizing Tensorflow for mobile\n",
    "\n",
    "### Choose from ready-made ML models\n",
    "- Vision,Translation,Speech, Natural Language\n",
    "\n",
    "### Customize ready-made ML models\n",
    "- Auto-ML\n",
    "\n",
    "### Brain, train, and serve, your own custom ML Models\n",
    "- ML Engine\n",
    "-  Storage, Bigquery, Datalab, Model Management, Pipelines\n",
    "\n",
    "### ML runtimes in a cloud-native environment\n",
    "1. Prototype with Cloud Datalab or Deep Learning Image\n",
    "2. Distribute and autoscale training and predictions with Cloud ML Engine\n",
    "\n",
    "### you may not be able to do machine learning solely on Google Cloud\n",
    "- Tied to On-Premise Infrastructure\n",
    "- Multi Cloud System Architecture\n",
    "- Running ML on the edge\n",
    "\n",
    "### Kubernetes minimizes infrastructure management\n",
    "### Kubeflow enable hybrid machine learning\n",
    "- GKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "premise (주장의) 전제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-1. KubeFlow\n",
    "\n",
    "## Machine Learning on Hybrid Cloud\n",
    "- Composability\n",
    "- Portability\n",
    "- Scalability\n",
    "\n",
    "## Composability\n",
    "- Build a model\n",
    "\n",
    "### Building a model is only one part of the entire system\n",
    "- 5% build a model, rest are 95%\n",
    "\n",
    "### Composability is about microservices\n",
    "\n",
    "## Portability\n",
    "- Experimentation \n",
    " - Model,UX, Tooling, Framework, Storage, Runtime, Drivers, OS\n",
    "- Training\n",
    "- Cloud\n",
    "\n",
    "### \"Portability are not problem\" wrong\n",
    "- it is essential\n",
    "\n",
    "### Your Labtop Counts\n",
    "\n",
    "## Scalability\n",
    "- More accelerators(GPU,TPU)\n",
    "- More CPUs\n",
    "- More disk/networking\n",
    "- More skillsets(data engineers, data scientist)\n",
    "- More teams\n",
    "- More experiments\n",
    "\n",
    "# KubeFlow\n",
    "\n",
    "### Oh you want to use ML on K8s?\n",
    "First become an expert in:\n",
    "- Containers\n",
    "- Packaging\n",
    "- Kubernetes service endpoints\n",
    "- Persistent volumes\n",
    "- Scaling\n",
    "- Immutable deployments\n",
    "- GPUs, Drivers & the GPL\n",
    "- Cloud APIs\n",
    "- DevOps\n",
    "\n",
    "### Make it Easy for Everyone to Develop, Deploy and Mange Portable, Distributed ML on Kubernets\n",
    "\n",
    "### What's in the box?\n",
    "- Jupyter notebook\n",
    "- Multi-architecture, distributed training\n",
    "- Multi-framework model serving\n",
    "- Examples and walkthroughs for getting started\n",
    "- Ksonnet packaging for customizing it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composability : 점진적으로 새로운 기능을 더할 수 있는 웹서비스를 가능하게 하는 장비<br>\n",
    "sonnet : 소네트(10개의 음절로 구성되는 시행 14개가 일정한 운율로 이어지는 14행시)<br>\n",
    "asynchronous : 동시에 존재하지 않는<br>\n",
    "deploy 1. (군대·무기를) 배치하다   2. 효율적으로 사용하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/amygdala/code-snippets/tree/master/ml/kubeflow-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ease 1. 쉬움, 용이함, 편의성   2. (근심걱정 없이) 편안함, 안락함   3. 편해지다;<br>\n",
    "render  1. (어떤 상태가 되게) 만들다   2. (특히 어떤 것에 대한 대가로 또는 기대에 따라) 주다<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Benefits\n",
    "- Portability\n",
    "- Composability and reproducibility\n",
    "- Scalability\n",
    "- Visualization and Collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiaing Tensorflow for mobile\n",
    "\n",
    "### Increasingly, applications are combining ML with mobile apps\n",
    "- Image/OCR\n",
    "- Speech <=> Text\n",
    "- Translation\n",
    "\n",
    "### ML models can help extract meaning from raw data, thus reducing network traffic\n",
    "- Image recognition: send raw image v, send detected label\n",
    "- Motion detection: send raw motion v. send feature vector\n",
    "\n",
    "### From mobile devices, we often can't use the microservices approach\n",
    "- Monlithic Service\n",
    "- Microservice\n",
    " - Microservices can add unwanted latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "superimpose 1. (이미지를 결합하여 보여줄 수 있도록) 겹쳐 놓다   2. (어떤 요소·특질을) 덧붙이다<br>\n",
    "Monolithic 1. 하나의 암석으로 된; [건축] 중공 초석의  2. <조직·단결 등이> 단일체의, 한 덩어리로 뭉친  3. 획일적이고 자유가 없는 <사회>  4. (전자) 단일 결정(結晶)으로 된 <칩>, 모놀리식의 <회로> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow supports multiple mobile platforms\n",
    "- Tensorflow Lite\n",
    " - Reduce code footprint\n",
    " - Quantization\n",
    " - Lower precision arithmetic\n",
    " \n",
    "### Build with Bazel by starting with a git clone\n",
    "### Cocoapods support for iOS\n",
    "### Understand how to Code with the API\n",
    "### Even though we have talked primarily about prediction on mobile, a new frontier is federated learning\n",
    "\n",
    "### Large neural network can be compressed\n",
    "### There are several methods to reduce model size\n",
    "- Freeze graph\n",
    "- Transform the graph\n",
    "- Quantize weights and calculations\n",
    "\n",
    "### Freezing a graph can do load time optimization\n",
    "- Converts variables to constants and remove checkpoints\n",
    "### Transform your graph to remove nodes you don't use in prediction\n",
    "- strip_unused_nodes:\n",
    " - Remove training-only operations\n",
    "- fold_batch_norms:\n",
    " - Remove Muls for batch norm\n",
    "- quantize_weights quantize_nodes\n",
    " - Add quantization\n",
    "\n",
    "### Quantizing weights and calculations boosts performance\n",
    "### Tensorflow Lite is optimized for mobile apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which of these are reasons why you may not be able to machine learning solely on Google Cloud?\n",
    "> You are tied to on-premises or multi-cloud infrastructure due to business reasons,You need to run inference on the edge, TensorFlow is not supported on Google Cloud\n",
    ">>  You are tied to on-premises or multi-cloud infrastructure due to business reasons,You need to run inference on the edge\n",
    "\n",
    "2. A key principle behind Kubeflow is portability so that you can:\n",
    "> Move your model from on-prem to Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Build hybrid cloud machine learning models\n",
    "- Optimize Tensorflow graph for mobile\n",
    "\n",
    "\n",
    "# Summary\n",
    "## Agenda\n",
    "- Architecting Production ML Systems\n",
    "- Ingesting data for Cloud-based analytics and ML\n",
    "- Designing Adaptable ML systems\n",
    "- Designing High Performance ML Systems\n",
    "- Hybrid ML Systems\n",
    "\n",
    "### Training and Serving Decision\n",
    "- cloud function and add engine, cloud dataflow\n",
    "\n",
    "### Data Migration Options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cloud.google.com/blog/products/gcp/simplifying-machine-learning-on-open-hybrid-clouds-with-kubeflow<br>\n",
    "https://cloud.google.com/ml-engine/docs/tensorflow/technical-overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20a",
   "language": "python",
   "name": "tf20a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
