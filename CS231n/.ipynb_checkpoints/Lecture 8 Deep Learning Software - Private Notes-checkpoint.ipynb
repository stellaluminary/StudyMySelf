{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8 Deep Learning Software - Private Notes\n",
    " - Caffe, Torch, Theano, TensorFlow, Keras, PyTorch, etc<br>\n",
    "\n",
    "Slides : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture8.pdf<br>\n",
    "videos : https://www.youtube.com/watch?v=6SlgtELqOWc&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today\n",
    " - CPU vs GPU\n",
    " - Deep Learning FrameWorks\n",
    "  - Caffe / Caffe2\n",
    "  - Theano / TensorFlow\n",
    "  - Torch / PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming GPU\n",
    " - CUDA(NVIDIA only)\n",
    "  - Write C-like code that runs directly on the GPU\n",
    "  - Higher-level API:cuBLAS, cuFFT, cuDNN etc\n",
    " - OpenCL\n",
    "  - Similar to CUDA, but runs on anything\n",
    "  - Usually slower\n",
    " - Udacity: Intro to Parallel Programming\n",
    "  - https://www.udacity.com/course/cs344\n",
    "  - For deep learning just use existing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/8-21.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The point of deep learning frameworks\n",
    " (1) Easily build big computational graphs<br>\n",
    " (2) Easily compute gradients in computational graphs<br>\n",
    " (3) Run it all efficiently on GPU (wrap cuDNN, cuBLAS, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems\n",
    " - Can't run on GPU\n",
    " - Have to compute our own gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "x=np.random.randn(N,D)\n",
    "y=np.random.randn(N,D)\n",
    "z=np.random.randn(N,D)\n",
    "\n",
    "a=x*y\n",
    "b=a+z\n",
    "c=np.sum(b)\n",
    "\n",
    "grad_c = 1.\n",
    "grad_b = grad_c * np.ones((N,D))\n",
    "grad_a = grad_b.copy()\n",
    "grad_z = grad_b.copy()\n",
    "grad_x = grad_a * y\n",
    "grad_y = grad_a * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "x=tf.placeholder(tf.float32)\n",
    "y=tf.placeholder(tf.float32)\n",
    "z=tf.placeholder(tf.float32)\n",
    "\n",
    "a=x*y\n",
    "b=a+z\n",
    "c=tf.reduce_sum(b)\n",
    "\n",
    "grad_x, grad_y, grad_z = tf.gradients(c,[x,y,z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x:np.random.randn(N,D),\n",
    "        y:np.random.randn(N,D),\n",
    "        z:np.random.randn(N,D),\n",
    "    }\n",
    "    \n",
    "    out = sess.run([c,grad_x,grad_y,grad_z], feed_dict=values)\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D = 3,4\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    x=tf.placeholder(tf.float32)\n",
    "    y=tf.placeholder(tf.float32)\n",
    "    z=tf.placeholder(tf.float32)\n",
    "\n",
    "    a=x*y\n",
    "    b=a+z\n",
    "    c=tf.reduce_sum(b)\n",
    "\n",
    "grad_x, grad_y, grad_z = tf.gradients(c,[x,y,z])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {\n",
    "        x:np.random.randn(N,D),\n",
    "        y:np.random.randn(N,D),\n",
    "        z:np.random.randn(N,D),\n",
    "    }\n",
    "    \n",
    "    out = sess.run([c,grad_x,grad_y,grad_z], feed_dict=values)\n",
    "    c_val, grad_x_val, grad_y_val, grad_z_val = out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "```Python\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N,D=3,4\n",
    "\n",
    "x=Variable(torch.randn(N,D), requires_grad=True)\n",
    "y=Variable(torch.randn(N,D), requires_grad=True)\n",
    "z=Variable(torch.randn(N,D), requires_grad=True)\n",
    "\n",
    "\"\"\"\n",
    "x=Variable(torch.randn(N,D).cuda(), requires_grad=True)\n",
    "y=Variable(torch.randn(N,D).cuda(), requires_grad=True)\n",
    "z=Variable(torch.randn(N,D).cuda(), requires_grad=True)\n",
    "\"\"\"\n",
    "\n",
    "a=x*y\n",
    "b=a+z\n",
    "c=torch.sum(b)\n",
    "\n",
    "c.backward()\n",
    "\n",
    "print(x.grad.data)\n",
    "print(y.grad.data)\n",
    "print(z.grad.data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "# Running example: Train a two-layer ReLU network on random data with L2 loss\n",
    "\n",
    "# First define computational graph\n",
    "N,D,H=64,1000,100\n",
    "\n",
    "# Create placeholders for input x, weights w1 and w2, and targets y\n",
    "x=tf.placeholder(tf.float32, shape=(N,D))\n",
    "y=tf.placeholder(tf.float32, shape=(N,D))\n",
    "w1=tf.placeholder(tf.float32, shape=(D,H))\n",
    "w2=tf.placeholder(tf.float32, shape=(H,D))\n",
    "\n",
    "# Forward pass: compute prediction for y and loss (L2 distance between y and y_pred)\n",
    "# No computation happens here - just building the graph!\n",
    "h=tf.maximum(tf.matmul(x,w1),0)\n",
    "y_pred = tf.matmul(h,w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1))\n",
    "\n",
    "# Tell TensorFlow to compute loss of gradient with respect to w1 and w2.\n",
    "# Again no computation here - just building the graph\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1,w2])\n",
    "\n",
    "# Then run the graph many times\n",
    "# Now done building our graph, so we enter a session so we can actually run the graph\n",
    "with tf.Session() as sess:\n",
    "    # Create numpy arrays that will fill in the placeholders above\n",
    "    values = {x:np.random.randn(N,D),\n",
    "              w1:np.random.randn(D,H),\n",
    "              w2:np.random.randn(H,D),\n",
    "              y:np.random.randn(N,D),\n",
    "             }\n",
    "    \n",
    "    # Run the graph: feed in the numpy arrays for x, y, w1, and w2; get numpy arrays for loss, grad_w1, and grad_w2\n",
    "    out = sess.run([loss, grad_w1, grad_w2],\n",
    "                  feed_dict=values)\n",
    "    loss_val, grad_w1_val, grad_w2_val = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=(N,D))\n",
    "y=tf.placeholder(tf.float32, shape=(N,D))\n",
    "\n",
    "\n",
    "w1=tf.placeholder(tf.float32, shape=(D,H))\n",
    "w2=tf.placeholder(tf.float32, shape=(H,D))\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)\n",
    "y_pred = tf.matmul(h,w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1))\n",
    "\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1,w2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    values = {x:np.random.randn(N,D),\n",
    "              w1:np.random.randn(D,H),\n",
    "              w2:np.random.randn(H,D),\n",
    "              y:np.random.randn(N,D),\n",
    "             }\n",
    "    learning_rate = 1e-5\n",
    "    # Train the network: Run the graph over and over,use gradient to update weights\n",
    "    # Problem: copying weights between CPU / GPU each step\n",
    "    for t in range(50):        \n",
    "        out = sess.run([loss, grad_w1, grad_w2],\n",
    "                      feed_dict=values)\n",
    "        loss_val, grad_w1_val, grad_w2_val = out\n",
    "        values[w1] -= learning_rate * grad_w1_val\n",
    "        values[w2] -= learning_rate * grad_w2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=(N,D))\n",
    "y=tf.placeholder(tf.float32, shape=(N,D))\n",
    "# Change w1 and w2 from placeholder (fed on each call) to Variable (persists in the graph between calls)\n",
    "w1 = tf.Variable(tf.random_normal((D,H)))\n",
    "w2 = tf.Variable(tf.random_normal((H,D)))\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)\n",
    "y_pred = tf.matmul(h,w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1))\n",
    "\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1,w2])\n",
    "\n",
    "learning_rate = 1e-5\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run graph once to initialize w1 and w2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = {x:np.random.randn(N,D),\n",
    "              y:np.random.randn(N,D),}\n",
    "    # Run many times to train\n",
    "    for t in range(50):        \n",
    "        loss_val, = sess.run([loss], feed_dict=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=(N,D))\n",
    "y=tf.placeholder(tf.float32, shape=(N,D))\n",
    "# Change w1 and w2 from placeholder (fed on each call) to Variable (persists in the graph between calls)\n",
    "w1 = tf.Variable(tf.random_normal((D,H)))\n",
    "w2 = tf.Variable(tf.random_normal((H,D)))\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)\n",
    "y_pred = tf.matmul(h,w2)\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1))\n",
    "\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1,w2])\n",
    "\n",
    "learning_rate = 1e-5\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "# Add dummy graph node that depends on updates\n",
    "updates=tf.group(new_w1, new_w2)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run graph once to initialize w1 and w2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = {x:np.random.randn(N,D),\n",
    "              y:np.random.randn(N,D),}\n",
    "    # Run many times to train\n",
    "    for t in range(50):      \n",
    "        # Tell graph to compute dummy node\n",
    "        loss_val,_ = sess.run([loss, updates], feed_dict=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=(N,D))\n",
    "y=tf.placeholder(tf.float32, shape=(N,D))\n",
    "# Change w1 and w2 from placeholder (fed on each call) to Variable (persists in the graph between calls)\n",
    "w1 = tf.Variable(tf.random_normal((D,H)))\n",
    "w2 = tf.Variable(tf.random_normal((H,D)))\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)\n",
    "y_pred = tf.matmul(h,w2)\n",
    "\"\"\"\n",
    "diff = y_pred - y\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis=1))\n",
    "\"\"\"\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "#Can use an optimizer to compute gradients and update weights\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5)\n",
    "updates = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run graph once to initialize w1 and w2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = {x:np.random.randn(N,D),\n",
    "              y:np.random.randn(N,D),}\n",
    "    # Run many times to train\n",
    "    for t in range(50):      \n",
    "        # Tell graph to compute dummy node\n",
    "        # Remember to execute the output of the optimizer!\n",
    "        loss_val,_ = sess.run([loss, updates], feed_dict=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "\n",
    "x=tf.placeholder(tf.float32, shape=(N,D))\n",
    "y=tf.placeholder(tf.float32, shape=(N,D))\n",
    "\n",
    "#Use Xavier initializer\n",
    "init=tf.contrib.layers.xavier_initializer()\n",
    "#tf.layers automatically sets up weight and bias for us!\n",
    "h=tf.layers.dense(inputs=x, units=H, activation=tf.nn.relu, kernel_initializer=init)\n",
    "y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "#Can use an optimizer to compute gradients and update weights\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e0)\n",
    "updates = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run graph once to initialize w1 and w2\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = {x:np.random.randn(N,D),\n",
    "              y:np.random.randn(N,D),}\n",
    "    # Run many times to train\n",
    "    for t in range(50):      \n",
    "        # Tell graph to compute dummy node\n",
    "        # Remember to execute the output of the optimizer!\n",
    "        loss_val,_ = sess.run([loss, updates], feed_dict=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras: High - Level Wrapper\n",
    " - Keras is a layer on top of Tensorflow, makes common things easy to do<br>\n",
    "(Also supports Theano backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "N,D,H = 64,1000,100\n",
    "\n",
    "# Define model object as a sequence of layers\n",
    "model=Sequential()\n",
    "#model.add(Dense(input_dim=D, output_dim=H))\n",
    "model.add(Dense(input_dim=D, units=H))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(input_dim=H, units=D))\n",
    "\n",
    "# Define optimizer object\n",
    "optimizer = SGD(lr=1e0)\n",
    "# Build the model,specify loss function\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "x = np.random.randn(N,D)\n",
    "y = np.random.randn(N,D)\n",
    "\n",
    "# Train the model with a single line!\n",
    "#history = model.fit(x,y, nb_epoch=50, batch_size=N, verbose=0)\n",
    "history = model.fit(x,y, epochs=50, batch_size=N, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: Other High-Level Wrappers \n",
    "- Keras (https://keras.io/)\n",
    "- TFLearn (http://tflearn.org/)\n",
    "\n",
    "- TensorLayer (http://tensorlayer.readthedocs.io/en/latest/) #Ships with Tensorflow\n",
    "- tf.layers (https://www.tensorflow.org/api_docs/python/tf/layers) #Ships with Tensorflow\n",
    "- TF-Slim (https://github.com/tensorflow/models/tree/master/inception/inception/slim) #Ships with Tensorflow\n",
    "- tf.contrib.learn (https://www.tensorflow.org/get_started/tflearn) #From google\n",
    "- Pretty Tensor (https://github.com/google/prettytensor) # From DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow: Pretrained Models\n",
    "- TF-Slim: (https://github.com/tensorflow/models/tree/master/slim/nets)\n",
    "- Keras: (https://github.com/fchollet/deep-learning-models)\n",
    "\n",
    "# TensorFlow: Tensorboard\n",
    "- Add logging to code to record loss, stats, etc \n",
    "- Run server and get pretty graphs!\n",
    "\n",
    "# TensorFlow: Distributed Version\n",
    "- Split one graph over multiple machines!\n",
    "- https://www.tensorflow.org/deploy/distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side Note: Theano\n",
    "\n",
    "```Python\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "N,D,H,C = 64,1000,100,10\n",
    "\n",
    "# Define symbolic variables (similar to TensorFlow placeholder)\n",
    "x=T.matrix('x')\n",
    "y=T.vector('y',dtype='int64')\n",
    "w1 = T.matrix('w1')\n",
    "w2=T.matrix('w2')\n",
    "\n",
    "# Forward pass: compute predictions and loss\n",
    "a=x.dot(w1)\n",
    "a_relu=T.nnet.relu(a)\n",
    "scores = a_relu.dot(w2)\n",
    "\n",
    "# Forward pass: compute predictions and loss (no computation performed yet)\n",
    "probs = T.nnet.softmax(scores)\n",
    "loss = T.nnet.categorical_crossentropy(probs,y).mean()\n",
    "\n",
    "# Ask Theano to compute gradients for us (no computation performed yet)\n",
    "dw1,dw2 = T.grad(loss,[w1,w2])\n",
    "\n",
    "# Compile a function that computes loss, scores, and gradients from data and weights\n",
    "f=theano.funtion(\n",
    "    inputs=[x,y,w1,w2],\n",
    "    outputs=[loss,scores,dw1,dw2],\n",
    ")           \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythorch : Three Levels ofAbstraction\n",
    " - Tensor:Imperative ndarray, but runs on GPU\n",
    " - Variable: Node in a computational graph; stores data and gradient\n",
    " - Modules: A neural network layer; may store state or learnable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensors\n",
    "PyTorch Tensors are just like numpy arrays, but they can run on GPU.<br>\n",
    "No built-in notion of computational graph, or gradients, or deep learning.<br>\n",
    "Here we fit a two-layer net using PyTorch Tensors:<br>\n",
    "```Python\n",
    "import torch\n",
    "\n",
    "# To run on GPU, just cast tensors to a cuda datatype!\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# Create random tensors for data and weights\n",
    "N, D_in, H, D_out = 64,1000,100,10\n",
    "x=torch.randn(N,D_in).type(dtype)\n",
    "y=torch.randn(N,D_out).type(dtype)\n",
    "w1=torch.randn(D_in, H).type(dtype)\n",
    "w2=torch.randn(H,D_out).type(dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predictions and loss\n",
    "    h=x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    loss=(y_pred - y).pow(2).sum()\n",
    "    \n",
    "    # Backward pass: manually compute gradients\n",
    "    grad_y_pred = 2. * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # Gradient descent step on weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Autograd\n",
    "A PyTorch Variable is a node in a computational graph <br>\n",
    "x.data is a Tensor <br>\n",
    "x.grad is a Variable of gradients <br>\n",
    "(same shape as x.data) <br>\n",
    "x.grad.data is a Tensor of gradients <br>\n",
    "\n",
    "```Python\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create random tensors for data and weights\n",
    "N, D_in, H, D_out = 64,1000,100,10\n",
    "\n",
    "# We will not want gradients (of loss) with respect to data\n",
    "x=Variable(torch.randn(N,D_in), requires_grad=False)\n",
    "y=Variable(torch.randn(N,D_out), requires_grad=False)\n",
    "# Do want gradients with respect to weights\n",
    "w1=Variable(torch.randn(D_in, H), requires_grad=True)\n",
    "w2=Variable(torch.randn(H,D_out), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    \n",
    "    # Compute gradient of loss with respect to w1 and w2 (zero out grads first)\n",
    "    if w1.grad: w1.grad.data.zero_()\n",
    "    if w2.grad: w2.grad.data.zero_()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Make gradient step on weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "        \n",
    "```\n",
    "PyTorch Tensors and Variables have the same API! <br>\n",
    "Variables remember how they were created (for backprop) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: New Autograd Functions\n",
    "- (slides:94)Define your own autograd functions by writing forward and backward for Tensors <br>\n",
    "(similar to modular layers in A2)\n",
    "- Can use our new autograd function in the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: nn\n",
    "- Higher-level wrapper for working with neural nets\n",
    "- Similar to Keras and friends but only one, and it’s good =)\n",
    "- Define our model as a sequence of layers\n",
    "- nn also defines common loss functions\n",
    "- Forward pass: feed data to model, and prediction to loss function\n",
    "- Backward pass: compute all gradients\n",
    "- Make gradient step on each model parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: optim\n",
    "- Use an optimizer for different update rules\n",
    "- Update all parameters after computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: nn Define new Modules\n",
    "- A PyTorch Module is a neural net layer; it inputs and outputs Variables\n",
    "- Modules can contain weights (as Variables) or other Modules\n",
    "- You can define your own Modules using autograd!\n",
    "- Define our whole model as a single Module\n",
    "- Initializer sets up two children (Modules can contain modules)\n",
    "- Define forward pass using child modules and autograd ops on Variables\n",
    "- No need to define backward - autograd will handle it\n",
    "- Construct and train an instance of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: DataLoaders\n",
    "- A DataLoader wraps a Dataset and provides minibatching, shuffling, multithreading, for you\n",
    "- When you need to load custom data, just write your own Dataset class\n",
    "- Iterate over loader to form minibatches\n",
    "- Loader gives Tensors so you need to wrap in Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Pretrained Models\n",
    "- Super easy to use pretrained models with torchvision\n",
    "- https://github.com/pytorch/vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Visdom\n",
    "- Somewhat similar to TensorBoard: add logging to your code, then visualized in a browser\n",
    "- Can’t visualize computational graph structure (yet?)\n",
    "- https://github.com/facebookresearch/visdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: Torch\n",
    "- Direct ancestor of PyTorch (they share a lot of C backend)\n",
    "- Written in Lua, not Python\n",
    "- PyTorch has 3 levels of abstraction: Tensor, Variable, and Module\n",
    "- Torch only has 2: Tensor, Module\n",
    "- More details: Check 2016 slides\n",
    "\n",
    "- Build a model as a sequence of layers, and a loss function\n",
    "- Forward: compute scores and loss\n",
    "- Backward: compute gradient (no autograd, need to pass grad_scores around)\n",
    "- Define a callback that inputs weights, produces loss and gradient on weights\n",
    "- Pass callback to optimizer over and over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch vs PyTorch\n",
    "    Torch :   (-) Lua    (-) No autograd (+) More stable           (+) Lots of existing code (0) Fast\n",
    "    PyTorch : (+) Python (+) Autograd    (-) Newer, still changing (-) Less existing code    (0) Fast\n",
    "    \n",
    "## Conclusion: Probably use PyTorch for new projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/8-120.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - With static graphs,framework can optimize the graph for you before it runs!\n",
    " - Static : Once graph is built, can serialize it and run it without the code that built the graph!\n",
    " - Dynamic : Graph building and execution are intertwined, so always need to keep code around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/8-125.png'>\n",
    "<img src='./Lesson pic/8-128.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Graphs in TensorFlow\n",
    "- TensorFlow Fold make dynamic graphs easier in TensorFlow through dynamic batching\n",
    "- Recurrent networks\n",
    "- Recursive networks\n",
    "- Modular Networks\n",
    "- (Your creative idea here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe (UC Berkeley)\n",
    " - Core written in C++\n",
    " -  Has Python and MATLAB bindings\n",
    " -  Good for training or finetuning feedforward classification models\n",
    " -  Often no need to write code!\n",
    " -  Not used as much in research anymore, still popular for deploying models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe: Training / Finetuning\n",
    "No need to write code!\n",
    "1. Convert data (run a script)\n",
    "2. Define net (edit prototxt)\n",
    "3. Define solver (edit prototxt)\n",
    "4. Train (with pretrained weights) (run a script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe step 1: Convert Data\n",
    "- ● DataLayer reading from LMDB is the easiest\n",
    "- ● Create LMDB using convert_imageset\n",
    "- ● Need text file where each line is\n",
    " - ○ “[path/to/image.jpeg] [label]”\n",
    "- ● Create HDF5 file yourself using h5py\n",
    "- ● ImageDataLayer: Read from image files\n",
    "- ● WindowDataLayer: For detection\n",
    "- ● HDF5Layer: Read from HDF5 file\n",
    "- ● From memory, using Python interface\n",
    "- ● All of these are harder to use (except Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe step 2: Define Network (prototxt)\n",
    "- ● .prototxt can get ugly for big models\n",
    "- ● ResNet-152 prototxt is 6775 lines long!\n",
    "- ● Not “compositional”; can’t easily define a residual block and reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe step 3: Define Solver (prototxt)\n",
    "- ● Write a prototxt file defining a SolverParameter\n",
    "- ● If finetuning, copy existing solver.prototxt file\n",
    "  - ○ Change net to be your net\n",
    "  - ○ Change snapshot_prefix to your output\n",
    "  - ○ Reduce base learning rate(divide by 100)\n",
    "  - ○ Maybe change max_iter and snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe step 4: Train!\n",
    "```\n",
    "./build/tools/caffe train \\\n",
    "-gpu 0 \\\n",
    "-model path/to/trainval.prototxt \\\n",
    "-solver path/to/solver.prototxt \\\n",
    "-weights path/to/pretrained_weights.caffemodel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead of -gpu 0 \\ You Could Write\n",
    "-gpu -1 for CPU-only <br>\n",
    "-gpu all for multi-gpu <br>\n",
    "https://github.com/BVLC/caffe/blob/master/tools/caffe.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caffe Model Zoo\n",
    "\n",
    "    AlexNet, VGG, GoogLeNet, ResNet, plus others\n",
    "\n",
    "    https://github.com/BVLC/caffe/wiki/Model-Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe Python Interface\n",
    "Not much documentation…<br>\n",
    "Read the code! Two most important files:<br>\n",
    "    ● caffe/python/caffe/_caffe.cpp: https://github.com/BVLC/caffe/blob/master/python/caffe/_caffe.cpp <br>\n",
    "        ○ Exports Blob, Layer, Net, and Solver classes<br>\n",
    "    ● caffe/python/caffe/pycaffe.py : https://github.com/BVLC/caffe/blob/master/python/caffe/pycaffe.py<br>\n",
    "        ○ Adds extra methods to Net class<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good for:<br>\n",
    "● Interfacing with numpy<br>\n",
    "● Extract features: Run net forward<br>\n",
    "● Compute gradients: Run net backward (DeepDream, etc)<br>\n",
    "● Define layers in Python with numpy (CPU only)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe Pros / Cons\n",
    "\n",
    "● (+) Good for feedforward networks<br>\n",
    "● (+) Good for finetuning existing networks<br>\n",
    "● (+) Train models without writing any code!<br>\n",
    "● (+) Python interface is pretty useful!<br>\n",
    "● (+) Can deploy without Python<br>\n",
    "● (-) Need to write C++ / CUDA for new GPU layers<br>\n",
    "● (-) Not good for recurrent networks<br>\n",
    "● (-) Cumbersome for big networks (GoogLeNet, ResNet)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe2 (Facebook)\n",
    "● Very new - released a week ago =) <br>\n",
    "● Static graphs, somewhat similar to TensorFlow<br>\n",
    "● Core written in C++<br>\n",
    "● Nice Python interface<br>\n",
    "● Can train model in Python, then serialize and deploy without Python <br>\n",
    "● Works on iOS / Android, etc <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google:  \n",
    "TensorFlow (“One framework to rule them all”)\n",
    "\n",
    "# Facebook:\n",
    "PyTorch(Research) +Caffe2(Production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Advice:\n",
    "TensorFlow is a safe bet for most projects. Not perfect but has<br>\n",
    "huge community, wide usage. Maybe pair with high-level wrapper<br>\n",
    "(Keras, Sonnet, etc)<br>\n",
    "I think PyTorch is best for research. However still new, there can be<br>\n",
    "rough patches.<br>\n",
    "Use TensorFlow for one graph over many machines<br>\n",
    "Consider Caffe, Caffe2, or TensorFlow for production deployment<br>\n",
    "\n",
    "Consider TensorFlow or Caffe2 for mobile<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/8-120.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
