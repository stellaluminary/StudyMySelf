{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9 CNN Architectures - Private Notes\n",
    "\n",
    "- AlexNet, VGG, GoogLeNet, ResNet, etc \n",
    "- slides : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf\n",
    "- Videos : https://www.youtube.com/watch?v=DAOcjicFr1Y&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today: CNN Architectures\n",
    "\n",
    "## Case Studies\n",
    "- AlexNet\n",
    "- VGG\n",
    "- GoogLeNet\n",
    "- ResNet\n",
    "\n",
    "## Also....\n",
    "- NiN (Network in Network)\n",
    "- Wide ResNet\n",
    "- ResNeXT\n",
    "- Stochastic Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: LeNet-5 [LeCun et al., 1998]\n",
    "\n",
    "Conv filters were 5x5, applied at stride 1<br>\n",
    "Subsampling (Pooling) layers were 2x2 applied at stride 2<br>\n",
    "i.e. architecture is [CONV-POOL-CONV-POOL-FC-FC]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: AlexNet [Krizhevsky et al. 2012]\n",
    "First CNN-based winner\n",
    "## Architecture:\n",
    "CONV1 - MAX POOL1 - NORM1 <br>\n",
    "CONV2 - MAX POOL2 - NORM2 <br>\n",
    "CONV3 - CONV4 - CONV5 - Max POOL3 <br>\n",
    "FC6 - FC7 - FC8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: 227x227x3 images \n",
    "### First layer (CONV1): 96 11x11 filters applied at stride 4\n",
    "\n",
    "Q: what is the output volume size? Hint: (227-11)/4+1 = 55 <br>\n",
    "=> Output volume [55x55x96]<br>\n",
    "Q: What is the total number of parameters in this layer?<br>\n",
    "=> Parameters: (11*11*3)*96 = 35K<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input: 227x227x3 images\n",
    "### After CONV1: 55x55x96\n",
    "\n",
    "Second layer (POOL1): 3x3 filters applied at stride 2<br>\n",
    "Q: what is the output volume size? Hint: (55-3)/2+1 = 27<br>\n",
    "=> Output volume: 27x27x96<br>\n",
    "Q: what is the number of parameters in this layer?<br>\n",
    "=> 0!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full (simplified) AlexNet architecture:<br>\n",
    "[227x227x3] INPUT<br>\n",
    "[55x55x96] CONV1: 96 11x11 filters at stride 4, pad 0<br>\n",
    "[27x27x96] MAX POOL1: 3x3 filters at stride 2<br>\n",
    "[27x27x96] NORM1: Normalization layer<br>\n",
    "[27x27x256] CONV2: 256 5x5 filters at stride 1, pad 2<br>\n",
    "[13x13x256] MAX POOL2: 3x3 filters at stride 2<br>\n",
    "[13x13x256] NORM2: Normalization layer<br>\n",
    "[13x13x384] CONV3: 384 3x3 filters at stride 1, pad 1<br>\n",
    "[13x13x384] CONV4: 384 3x3 filters at stride 1, pad 1<br>\n",
    "[13x13x256] CONV5: 256 3x3 filters at stride 1, pad 1<br>\n",
    "[6x6x256] MAX POOL3: 3x3 filters at stride 2<br>\n",
    "[4096] FC6: 4096 neurons<br>\n",
    "[4096] FC7: 4096 neurons<br>\n",
    "[1000] FC8: 1000 neurons (class scores)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details/Retrospectives:\n",
    "- first use of ReLU\n",
    "- used Norm layers (not common anymore)\n",
    "- heavy data augmentation\n",
    "- dropout 0.5\n",
    "- batch size 128\n",
    "- SGD Momentum 0.9\n",
    "- Learning rate 1e-2, reduced by 10\n",
    "manually when val accuracy plateaus\n",
    "- L2 weight decay 5e-4\n",
    "- 7 CNN ensemble: 18.2% -> 15.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical note: \n",
    "Trained on GTX 580 GPU with only 3 GB of memory.<br>\n",
    "Network spread across 2 GPUs, half the neurons (feature maps) on each GPU.<br>\n",
    "CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU. <br>\n",
    "CONV3, FC6, FC7, FC8: Connections with all feature maps in preceding layer, communication across GPUs<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-23.png'>\n",
    "<img src='./Lesson pic/9-24.png'>\n",
    "<img src='./Lesson pic/9-25.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: VGGNet [Simonyan and Zisserman, 2014]\n",
    "\n",
    "Small filters, Deeper networks<br>\n",
    "\n",
    "8 layers (AlexNet) -> 16 - 19 layers (VGG16Net)<br>\n",
    "Only 3x3 CONV stride 1, pad 1 and 2x2 MAX POOL stride 2<br>\n",
    "11.7% top 5 error in ILSVRC’13 (ZFNet) <br>\n",
    "-> 7.3% top 5 error in ILSVRC’14<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-26.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Why use smaller filters? (3x3 conv)<br>\n",
    "\n",
    "Stack of three 3x3 conv (stride 1) layers has same effective receptive field <br>\n",
    "as one 7x7 conv layer\n",
    "\n",
    "Q: What is the effective receptive field of three 3x3 conv (stride 1) layers?<br>\n",
    "[7x7]<br>\n",
    "\n",
    "But deeper, more non-linearities<br>\n",
    "\n",
    "And fewer parameters: 3 * (32C2) vs.72C2 for C channels per layer<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG 16(not counting biases) \n",
    "INPUT: [224x224x3] memory: 224*224*3=150K params: 0<br>\n",
    "CONV3-64: [224x224x64] memory: 224*224*64=3.2M params: (3*3*3)*64 = 1,728<br>\n",
    "CONV3-64: [224x224x64] memory: 224*224*64=3.2M params: (3*3*64)*64 = 36,864<br>\n",
    "POOL2: [112x112x64] memory: 112*112*64=800K params: 0<br>\n",
    "CONV3-128: [112x112x128] memory: 112*112*128=1.6M params: (3*3*64)*128 = 73,728<br>\n",
    "CONV3-128: [112x112x128] memory: 112*112*128=1.6M params: (3*3*128)*128 = 147,456<br>\n",
    "POOL2: [56x56x128] memory: 56*56*128=400K params: 0<br>\n",
    "CONV3-256: [56x56x256] memory: 56*56*256=800K params: (3*3*128)*256 = 294,912<br>\n",
    "CONV3-256: [56x56x256] memory: 56*56*256=800K params: (3*3*256)*256 = 589,824<br>\n",
    "CONV3-256: [56x56x256] memory: 56*56*256=800K params: (3*3*256)*256 = 589,824<br>\n",
    "POOL2: [28x28x256] memory: 28*28*256=200K params: 0<br>\n",
    "CONV3-512: [28x28x512] memory: 28*28*512=400K params: (3*3*256)*512 = 1,179,648<br>\n",
    "CONV3-512: [28x28x512] memory: 28*28*512=400K params: (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [28x28x512] memory: 28*28*512=400K params: (3*3*512)*512 = 2,359,296<br>\n",
    "POOL2: [14x14x512] memory: 14*14*512=100K params: 0<br>\n",
    "CONV3-512: [14x14x512] memory: 14*14*512=100K params: (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [14x14x512] memory: 14*14*512=100K params: (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [14x14x512] memory: 14*14*512=100K params: (3*3*512)*512 = 2,359,296<br>\n",
    "POOL2: [7x7x512] memory: 7*7*512=25K params: 0<br>\n",
    "FC: [1x1x4096] memory: 4096 params: 7*7*512*4096 = 102,760,448<br>\n",
    "FC: [1x1x4096] memory: 4096 params: 4096*4096 = 16,777,216<br>\n",
    "FC: [1x1x1000] memory: 1000 params: 4096*1000 = 4,096,000<br>\n",
    "\n",
    "<font color='red'><b>TOTAL memory: 24M * 4 bytes ~= 96MB / image (only forward! ~*2 for bwd)</b></font><br>\n",
    "<font color='blue'><b>TOTAL params: 138M parameters</b></font>\n",
    "\n",
    "<b>Most memory is in early CONV </b><br>\n",
    "ex) CONV3-64: [224x224x64] memory: 224*224*64=3.2M params: (3*3*3)*64 = 1,728<br>\n",
    "<b>Most params are in late FC</b><br>\n",
    "ex) FC: [1x1x4096] memory: 4096 params: 7*7*512*4096 = 102,760,448<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details:\n",
    "- ILSVRC’14 2nd in classification, 1st in\n",
    "localization\n",
    "- Similar training procedure as Krizhevsky\n",
    "2012\n",
    "- No Local Response Normalisation (LRN)\n",
    "- Use VGG16 or VGG19 (VGG19 only\n",
    "slightly better, more memory)\n",
    "- Use ensembles for best results\n",
    "- FC7 features generalize well to other\n",
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: GoogLeNet [Szegedy et al., 2014]\n",
    "\n",
    "Deeper networks, with computational efficiency\n",
    "\n",
    "- 22 layers\n",
    "- Efficient “Inception” module\n",
    "- No FC layers\n",
    "- Only 5 million parameters!\n",
    " - 12x less than AlexNet\n",
    "- ILSVRC’14 classification winner (6.7% top 5 error)\n",
    "\n",
    "“Inception module”: design a good local network topology (network within a network) <br>\n",
    "and then stack these modules on top of each other\n",
    "\n",
    "<img src='./Lesson pic/9-38.png'>\n",
    "\n",
    "Apply parallel filter operations on the input from previous layer:\n",
    "- Multiple receptive field sizes for convolution (1x1, 3x3, 5x5)\n",
    "- Pooling operation (3x3)\n",
    "\n",
    "Concatenate all filter outputs together depth-wise\n",
    "\n",
    "Q: What is the problem with this? [Hint: Computational complexity]\n",
    "<img src='./Lesson pic/9-39.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What is the output size of the 1x1 conv, with 128 filters?<br>\n",
    "=> 28x28x128<br>\n",
    "\n",
    "Q2: What are the output sizes of all different filter operations?<br>\n",
    "=> 28x28x192, 28x28x192, 28x28x96, 28x28x256<br>\n",
    "\n",
    "Q3:What is output size after filter concatenation?<br>\n",
    "=> 28x28x(128+192+96+256) = 28x28x672<br>\n",
    "\n",
    "<b>Q: What is the problem with this? [Hint: Computational complexity]<br></b>\n",
    "\n",
    "Conv Ops:<br>\n",
    "[1x1 conv, 128] 28x28x128x1x1x256<br>\n",
    "[3x3 conv, 192] 28x28x192x3x3x256<br>\n",
    "[5x5 conv, 96] 28x28x96x5x5x256<br>\n",
    "<b>Total: 854M ops<br></b>\n",
    "\n",
    "Very expensive compute\n",
    "\n",
    "Pooling layer also preserves feature depth, <br>\n",
    "which means total depth after concatenation can only grow at every layer!<br>\n",
    "\n",
    "<font color='blue'><b>Solution: “bottleneck” layers that use 1x1 convolutions to reduce feature depth<br></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-50.png'>\n",
    "<img src='./Lesson pic/9-52.png'>\n",
    "\n",
    "<br>\n",
    "<font color='blue' size=5><b>\n",
    "(each filter has size 1x1x64, and performs a 64-dimensional dot product)<br>\n",
    "preserves spatial dimensions, reduces depth!<br>\n",
    "Projects depth to lower dimension (combination of feature maps)<br>\n",
    "</b></font><br>\n",
    "\n",
    "<img src='./Lesson pic/9-54.png'>\n",
    "<img src='./Lesson pic/9-55.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using same parallel layers as naive example, and adding “1x1 conv, 64 filter” bottlenecks:<br>\n",
    "\n",
    "<font color='blue'><b>Conv Ops:</b></font><br>\n",
    "[1x1 conv, 64] 28x28x64x1x1x256<br>\n",
    "[1x1 conv, 64] 28x28x64x1x1x256<br>\n",
    "[1x1 conv, 128] 28x28x128x1x1x256<br>\n",
    "[3x3 conv, 192] 28x28x192x3x3x64<br>\n",
    "[5x5 conv, 96] 28x28x96x5x5x64<br>\n",
    "[1x1 conv, 64] 28x28x64x1x1x256<br>\n",
    "<font color='blue'><b>Total: 358M ops</b></font><br>\n",
    "\n",
    "Compared to 854M ops for naive version<br>\n",
    "Bottleneck can also reduce depth after pooling layer<br>\n",
    "\n",
    "Stack Inception modules with dimension reduction on top of each other<br>\n",
    "\n",
    "<img src='./Lesson pic/9-55.png'>\n",
    "<img src='./Lesson pic/9-57.png'>\n",
    "<img src='./Lesson pic/9-58.png'>\n",
    "<img src='./Lesson pic/9-60.png'>\n",
    "<img src='./Lesson pic/9-61.png'>\n",
    "\n",
    "22 total layers with weights (including each parallel layer in an Inception module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-64.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: ResNet [He et al., 2015]\n",
    "\n",
    "Very deep networks using residual connections\n",
    "\n",
    "- 152-layer model for ImageNet\n",
    "- ILSVRC’15 classification winner (3.57% top 5 error)\n",
    "- Swept all classification and detection competitions in ILSVRC’15 and COCO’15!\n",
    "\n",
    "<img src='./Lesson pic/9-65.png'>\n",
    "<img src='./Lesson pic/9-67.png'>\n",
    "\n",
    "<font color='blue'><b>\n",
    "56-layer model performs worse on both training and test error<br>\n",
    "-> The deeper model performs worse, but it’s not caused by overfitting!<br>\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue' size=5><b>\n",
    "Hypothesis: the problem is an optimization problem, deeper models are harder to optimize <br>\n",
    "\n",
    "The deeper model should be able to perform at least as well as the shallower model.\n",
    "\n",
    "A solution by construction is copying the learned layers from the shallower model <br>\n",
    "and setting additional layers to identity mapping.<br>\n",
    "</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-72.png'>\n",
    "<img src='./Lesson pic/9-74.png'>\n",
    "<img src='./Lesson pic/9-76.png'>\n",
    "<img src='./Lesson pic/9-79.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ResNet in practice:\n",
    "- Batch Normalization after every CONV layer\n",
    "- Xavier/2 initialization from He et al.\n",
    "- SGD + Momentum (0.9)\n",
    "- Learning rate: 0.1, divided by 10 when validation error plateaus\n",
    "- Mini-batch size 256\n",
    "- Weight decay of 1e-5\n",
    "- No dropout used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "- Able to train very deep networks without degrading (152 layers on ImageNet, 1202 on Cifar)\n",
    "- Deeper networks now achieve lowing training error as expected\n",
    "- Swept 1st place in all ILSVRC and COCO 2015 competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-82.png'>\n",
    "<img src='./Lesson pic/9-85.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>VGG: Highest memory, most operations</b><br>\n",
    "<b>GoogLeNet:most efficient</b><br>\n",
    "<b>AlexNet: Smaller compute, still memory heavy, lower accuracy</b><br>\n",
    "<b>ResNet: Moderate efficiency depending on model, highest accuracy</b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-90.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/9-92.png'>\n",
    "<img src='./Lesson pic/9-93.png'>\n",
    "<img src='./Lesson pic/9-94.png'>\n",
    "<img src='./Lesson pic/9-95.png'>\n",
    "<img src='./Lesson pic/9-96.png'>\n",
    "<img src='./Lesson pic/9-97.png'>\n",
    "<img src='./Lesson pic/9-98.png'>\n",
    "<img src='./Lesson pic/9-99.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: CNN Architectures\n",
    "\n",
    "- VGG, GoogLeNet, ResNet all in wide use, available in model zoos\n",
    "- ResNet current best default\n",
    "- Trend towards extremely deep networks\n",
    "- Significant research centers around design of layer / skip connections and improving gradient flow\n",
    "- Even more recent trend towards examining necessity of depth vs width and residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src='./Lesson pic/8-21.png'>\n",
    "<font color='blue' size=5><b></b></font>\n",
    "<img src='./Lesson pic/9-25.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "tf20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
