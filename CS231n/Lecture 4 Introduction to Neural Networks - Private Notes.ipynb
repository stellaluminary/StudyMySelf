{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 : Backpropagation and Neural Networks\n",
    "Lecture 4 slides : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf<br>\n",
    "Lecture 4 vidoe : https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$s=f(s;W) = Wx$</font> &emsp; <font color='blue'>scores function</font> <br>\n",
    "<font size=4>$L_i = \\sum_{j\\neq y_i} max(0,s_j- s_{y_i}+1)$ </font>&emsp; <font color='blue'>SVM loss</font><br>\n",
    "<font size=4>$L = \\frac{1}{N} \\sum_{i=1}^N L_i + \\sum_k W_k^2$ </font> &emsp; <font color='blue'>Data Loss + Regularization</font><br>\n",
    "<font size=4>want $\\color{orange} {\\nabla_W L} $ </font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> code {background-color : #C9EBF9 !important;} </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style> code {background-color : #C9EBF9 !important;} </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vanilla Gradient Descent\n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_func, data, weights)\n",
    "    weight += -step_size*weight_Grad #perform parameter update\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Gradient descent</font><br><br>\n",
    "<font size=4>$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x)}{h}$</font><br>\n",
    "<b>Numerical gradient</b>:slow, approximate, easy to write<br>\n",
    "<b>Analytic gradient</b>: fast, exact, error-prone<br>\n",
    "In practice : Derive analytic gradient, check your implementation with numerical gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/4-8.png'>\n",
    "<img src='./Lesson pic/4-23.png'>\n",
    "<img src='./Lesson pic/4-29.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leverage 이점\n",
    "ex) leverage of chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/4-45.png'>\n",
    "<img src='./Lesson pic/4-50.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add gate : 같은 값을 분산하는 gradient distributor<br>\n",
    "max gate : z에는 1, w에는 0인 gradient router <br>\n",
    "mul gate : gradient로 다른 변수의 값을 가진다.gradient switcher<br>\n",
    "ex) f=xy일 때 y로의 backpropagatioin = [local gradient]x[upstream gradient] <br>\n",
    "= $\\frac {df}{dy} * [upstream gradient] = x * (-2) = 3 * (-2) = -6$<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>$\\frac{\\partial f}{\\partial x} = \\sum_i \\frac{\\partial f}{\\partial q_i} \\frac{\\partial q_i}{\\partial x}$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/4-52.png'>\n",
    "<img src='./Lesson pic/4-56.png'>\n",
    "<img src='./Lesson pic/4-57.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Matrix is diagonal Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$f(x,W) => \\{x \\Subset R^n | W \\Subset \\mathbb{R}^{n \\times n}\\}$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/4-65.png'>\n",
    "<img src='./Lesson pic/4-70.png'>\n",
    "<img src='./Lesson pic/4-74.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularized implementation: forward / backward API\n",
    "\n",
    "Graph (or Net) object (rough psuedo code)\n",
    "```python\n",
    "class ComputationalGraph(object):\n",
    "    def forward(inputs):\n",
    "        # 1. [pass inputs to input gate]\n",
    "        # 2. forward the computational graph:\n",
    "        for gate in self.graph.nodes_topologically_sorted():\n",
    "            gate.forward()\n",
    "        return loss\n",
    "    \n",
    "    def backward():\n",
    "        for gate in self.graph.nodes_topologically_sorted():\n",
    "            gate.backward() #littel piece of backprop(chain rule applied)\n",
    "        return inputs_gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex) z = x * y\n",
    "```python\n",
    "class MultiplyGate(object):\n",
    "    def forward(x,y):\n",
    "        z=x*y\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        return z\n",
    "    def backward(dz):\n",
    "        dx = self.y * dz #[dz/dx * dL/dz]\n",
    "        dy = self.x * dz #[dz/dy * dL/dz]\n",
    "        return [dx,dy]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary so far - Slides 4 - 81\n",
    " - <font size=4>neural nets will be very large : impractical to write down gradient formula by hand for all parameters\n",
    " - <b>backpropagation</b> = recursive application of the chain rule along a computational graph to compute the gradients of all inputs/ parameters/intermediates\n",
    " - implementations maintain a graph structure, where the nodes implement the <b>forward()/backward()</b> API\n",
    " - <b>Forward</b>: compute result of an operation and save any intermediates needed for gradient computation in memory\n",
    " - <b>Backward</b>: apply the chain rule to compute the gradient of the loss function with respect to the inputs </font><br>\n",
    "impractical 비실용적인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/4-86.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full implementation of training a 2-layer Neural Network needs ~ 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2383.3188180028683\n",
      "200 984.1325759813187\n",
      "300 514.4370355837139\n",
      "400 303.9939030338178\n",
      "500 195.3180066338283\n",
      "600 134.63161546130448\n",
      "700 97.68503941709682\n",
      "800 72.65913563294704\n",
      "900 54.99697183816089\n",
      "1000 42.08702861850945\n",
      "1100 32.85345755416939\n",
      "1200 26.162750447579246\n",
      "1300 21.112771917681698\n",
      "1400 17.18193342418582\n",
      "1500 14.040825425303332\n",
      "1600 11.543970856421858\n",
      "1700 9.54495968421416\n",
      "1800 7.9305878456433\n",
      "1900 6.623170338536777\n",
      "2000 5.555739589846581\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "N,D_in, H, D_out = 64,1000,100,10\n",
    "x,y = randn(N,D_in), randn(N, D_out)\n",
    "w1, w2 = randn(D_in, H), randn(H, D_out)\n",
    "\n",
    "for t in range(2000):\n",
    "    h = 1/(1+np.exp(-x.dot(w1)))\n",
    "    y_pred = h.dot(w2)\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if (t+1) %100 ==0:\n",
    "        print(t+1,loss)\n",
    "    \n",
    "    grad_y_pred = 2. * (y_pred - y)\n",
    "    grad_w2 = h.T.dot(grad_y_pred)\n",
    "    grad_h = grad_y_pred.dot(w2.T)\n",
    "    grad_w1 = x.T.dot(grad_h * h * (1-h))\n",
    "    \n",
    "    w1 -= 1e-4 * grad_w1\n",
    "    w2 -= 1e-4 * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/4-94.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Be very careful with your brain analogies!\n",
    "## Biological Neurons:\n",
    " - Many different types\n",
    " - Dendrites can perform complex non-linear computations\n",
    " - Synapses are not a single weight but a complex non-linear dynamical system\n",
    " - Rate code may not be adequate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Lesson pic/4-96.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    " - We arrange neurons into fully-connected layers\n",
    " - The abstraction of a layer has the nice property that it allows us to use efficient vectorized code (e.g. matrix multiplies)\n",
    " - Neural networks are not really neural\n",
    " - Next Time: Convolution Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
